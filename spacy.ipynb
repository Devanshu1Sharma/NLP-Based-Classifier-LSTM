# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import spacy
from spacy.util import minibatch, compounding
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from spacy import displacy
import explacy

# Ensure you have downloaded NLTK data files for stopwords
nltk.download('stopwords')

# Load spaCy model for adding TextCategorizer
nlp = spacy.load('en_core_web_sm')

# Load the datasets
train_df = pd.read_csv('train_data.csv')
test_df = pd.read_csv('test_data.csv')

# Sample 5000 records from training and 1000 from testing datasets for demonstration
train_df = train_df.head(5000)
test_df = test_df.head(1000)

# Exploratory Data Analysis
def eda(data, title='EDA'):
    plt.figure(figsize=(12, 6))
    sns.countplot(y='sub_category', data=data, order=data['sub_category'].value_counts().index)
    plt.title(f'{title}: Count of each Sub-Category')
    plt.xlabel('Count')
    plt.ylabel('Sub-Category')
    plt.show()

eda(train_df, title='Training Data EDA')
eda(test_df, title='Test Data EDA')

# Basic Cleaning Function
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Apply the preprocessing to both train and test data
train_df['crimeaditionalinfo'] = train_df['crimeaditionalinfo'].apply(preprocess_text)
test_df['crimeaditionalinfo'] = test_df['crimeaditionalinfo'].apply(preprocess_text)

# Stop words removal and stemming
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def clean_and_stem(text):
    tokens = text.split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

train_df['crimeaditionalinfo'] = train_df['crimeaditionalinfo'].apply(clean_and_stem)
test_df['crimeaditionalinfo'] = test_df['crimeaditionalinfo'].apply(clean_and_stem)

# Preparing data for spaCy's TextCategorizer
train_texts = train_df['crimeaditionalinfo'].tolist()
train_labels = [{'cats': {label: label == row['sub_category'] for label in train_df['sub_category'].unique()}} 
                for _, row in train_df.iterrows()]

# Add TextCategorizer to the spaCy pipeline
if 'textcat' not in nlp.pipe_names:
    textcat = nlp.add_pipe('textcat', last=True)
else:
    textcat = nlp.get_pipe('textcat')

# Add labels to TextCategorizer
for label in train_df['sub_category'].unique():
    textcat.add_label(label)

# Training spaCy model
n_iter = 5

# Disable other pipes during training to only train the TextCategorizer
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    for i in range(n_iter):
        losses = {}
        batches = minibatch(list(zip(train_texts, train_labels)), size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(texts, annotations, drop=0.5, losses=losses)
        print(f"Iteration {i + 1}, Losses: {losses}")

# Evaluate the model on test data
test_texts = test_df['crimeaditionalinfo'].tolist()
test_labels = test_df['sub_category'].tolist()

# Predictions and metrics
predictions = []
for doc in nlp.pipe(test_texts, disable=["ner", "parser"]):
    cats = doc.cats
    pred_label = max(cats, key=cats.get)
    predictions.append(pred_label)

# Convert labels to the original form for evaluation
predictions = [int(label) for label in predictions]

# Print metrics
print("Classification Report:")
print(classification_report(test_labels, predictions))

# Confusion Matrix
conf_matrix = confusion_matrix(test_labels, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Demonstrate parsing and NER with explacy and displaCy
example_text = train_df['crimeaditionalinfo'].iloc[0]

# Explacy parsing explanation
print("Explacy Parsing Explanation:")
explacy.print_parse_info(nlp, example_text)

# Visualize named entities with displaCy
print("Named Entity Recognition with displaCy:")
doc = nlp(example_text)
displacy.render(doc, style='ent', jupyter=True)
